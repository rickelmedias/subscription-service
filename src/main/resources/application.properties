# ===============================
# APPLICATION INFO
# ===============================
spring.application.name=subscription-api
server.port=8080

# ===============================
# DATABASE - H2 (Development - Default)
# ===============================
spring.datasource.url=jdbc:h2:mem:subscriptiondb
spring.datasource.driverClassName=org.h2.Driver
spring.datasource.username=sa
spring.datasource.password=

# ===============================
# H2 CONSOLE
# ===============================
spring.h2.console.enabled=true
spring.h2.console.path=/h2-console
spring.h2.console.settings.web-allow-others=false

# ===============================
# JPA / HIBERNATE
# ===============================
spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
spring.jpa.hibernate.ddl-auto=update
spring.jpa.show-sql=true
spring.jpa.properties.hibernate.format_sql=true
spring.jpa.properties.hibernate.use_sql_comments=true
spring.jpa.open-in-view=false

# ===============================
# LOGGING
# ===============================
logging.level.root=INFO
logging.level.com.example.subscription=DEBUG
logging.level.org.hibernate.SQL=DEBUG
logging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACE

# ===============================
# SWAGGER / OPENAPI
# ===============================
springdoc.api-docs.path=/api-docs
springdoc.swagger-ui.path=/swagger-ui.html
springdoc.swagger-ui.operationsSorter=method

# ===============================
# ACTUATOR (Health Check)
# ===============================
management.endpoints.web.exposure.include=health,info
management.endpoint.health.show-details=when-authorized
management.health.defaults.enabled=true

# ===============================
# OLLAMA / LANGCHAIN4J (AI Local)
# ===============================
# Configuração do Ollama rodando localmente com GPU AMD (ROCm)
# Certifique-se de que: 1) Ollama está rodando (ollama serve)
#                       2) Modelo foi baixado (ollama pull deepseek-coder:6.7b)

# URL do servidor Ollama (padrão: localhost:11434)
ollama.base-url=http://localhost:11434

# Modelo a ser usado (opções: deepseek-coder:6.7b, llama3:8b, mistral:7b)
ollama.model-name=deepseek-coder:6.7b

# Timeout em segundos para respostas do LLM
ollama.timeout-seconds=120

# Temperatura (0.0 = determinístico, 1.0 = criativo)
ollama.temperature=0.7

# ===============================
# RABBITMQ (Message Broker - Event-Driven Architecture)
# ===============================
# Conexão com RabbitMQ
spring.rabbitmq.host=localhost
spring.rabbitmq.port=5672
spring.rabbitmq.username=guest
spring.rabbitmq.password=guest
spring.rabbitmq.virtual-host=/

# Exchange e Filas
rabbitmq.exchange.name=gamification.events
rabbitmq.queue.course-completed=gamification.course.completed
rabbitmq.queue.notification=gamification.notification
rabbitmq.queue.analytics=gamification.analytics

# Routing Keys
rabbitmq.routing-key.course-completed=course.completed
rabbitmq.routing-key.notification=notification.#
rabbitmq.routing-key.analytics=analytics.#

